{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4f55716",
   "metadata": {},
   "source": [
    "# Making PotentialNet Graph Binary Files for Amazon SageMaker training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc00446",
   "metadata": {},
   "source": [
    "## Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fc4c66",
   "metadata": {},
   "source": [
    "In this example, we are going to build graph binary files out of PDBBind complex data. Two binary files are created from each PDB id. Each graph binary contains the bi-graph representing the complex of ligand and poket. The difference between two binary files are that one binary file contains a bi-graph used for the stage 1 of PotentialNet training while the other is used for the stage 2. \n",
    "\n",
    "* First, we are going to download PDBBind dataset (PDBBind v2020) from the PDBBind website. \n",
    "* Second, we are going to extract graph data from ligand (.sdf) and poket (.pdb) files and make binary files. \n",
    "* Finally, the binary files are splitted into 4 groups (train, validation, test, and additional holdout (i.e. fine-tuning)). CSV file which describes the instruction for split is also created. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd1ab50",
   "metadata": {},
   "source": [
    "## Dataset : PDBBind v2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff0baed",
   "metadata": {},
   "source": [
    "\n",
    "The following three archive files are downloaded from http://www.pdbbind.org.cn/.\n",
    "* CASF-2016.tar.gz (PDBBind v2020 core files)\n",
    "* PDBbind_v2020_refined.tar.gz (PDBBind v2020 refined files)\n",
    "* PDBbind_v2020_other_PL.tar.gz (PDBBind v2020 other files)\n",
    "\n",
    "In this example, these archive files have been already uploaded to S3. We are going to download them from S3 to the local directory (./data). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aed65ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = 'data'\n",
    "isExist = os.path.exists(data_dir)\n",
    "    \n",
    "if not isExist: \n",
    "    os.makedirs(data_dir)\n",
    "    print(f\"The new directory {data_dir} is created!\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c1a8ab",
   "metadata": {},
   "source": [
    "### Core Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2820e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://d2125kp0qwrvcx.cloudfront.net/dgl-pdbbind/CASF-2016.tar.gz -P ./{data_dir}\n",
    "#!aws s3 cp s3://'<your S3 prefix of PDBbind dataset>'/dgl-pdbbind/CASF-2016.tar.gz ./{data_dir}/CASF-2016.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13684055",
   "metadata": {},
   "source": [
    "### Refined Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9f1c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://d2125kp0qwrvcx.cloudfront.net/dgl-pdbbind/PDBbind_v2020_refined.tar.gz -P ./{data_dir}\n",
    "#!aws s3 cp s3://'<your S3 prefix of PDBbind dataset>'/dgl-pdbbind/PDBbind_v2020_refined.tar.gz ./{data_dir}/PDBbind_v2020_refined.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf93ce2",
   "metadata": {},
   "source": [
    "### Other Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d91142e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wget https://d2125kp0qwrvcx.cloudfront.net/dgl-pdbbind/PDBbind_v2020_other_PL.tar.gz -P ./{data_dir}\n",
    "#!aws s3 cp s3://'<your S3 prefix of PDBbind dataset>'/dgl-pdbbind/PDBbind_v2020_other_PL.tar.gz ./{data_dir}/PDBbind_v2020_other_PL.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d27ffc",
   "metadata": {},
   "source": [
    "PDBBind dataset has been modifying its indexing convention before. Datasets between v2016 and v2020 share the similar indexing convention. Likewise, datasets between v2012 and 2015 are similar among them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be3a9fb",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This is a example for building a conda environment to run this notebook on Amazon SageMaker notebook instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff971cc5",
   "metadata": {},
   "source": [
    "```\n",
    "conda create --name dgllife6 --clone pytorch_latest_p36\n",
    "conda activate dgllife6\n",
    "conda install -y -c dglteam rdkit==2018.09.3 dgl-cuda10.1==0.7.2\n",
    "conda list\n",
    "pip install dgllife==0.2.9\n",
    "pip install -U ipykernel\n",
    "pip uninstall -y ipython prompt_toolkit\n",
    "pip install ipython prompt_toolkit\n",
    "python -m ipykernel install --user --name dgllife6 --display-name dgllife6\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30246a10",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5c80b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from functools import partial\n",
    "from itertools import accumulate\n",
    "\n",
    "import dgl\n",
    "import numpy as np\n",
    "import numpy.random as nrd\n",
    "import torch\n",
    "from dgl.data.utils import Subset\n",
    "from dgllife.data.pdbbind import PDBBind#, PDBBind_v2020\n",
    "from dgllife.model import ACNN, PotentialNet\n",
    "from dgllife.utils import (PN_graph_construction_and_featurization,\n",
    "                           ACNN_graph_construction_and_featurization,#\n",
    "                           multiprocess_load_molecules)#\n",
    "# Added\n",
    "from dgl.data.utils import get_download_dir, download, _get_dgl_url, extract_archive#\n",
    "import pandas as pd\n",
    "import glob\n",
    "import dgl.backend as F\n",
    "import multiprocessing\n",
    "import os\n",
    "import glob\n",
    "from functools import partial\n",
    "\n",
    "# Save graph\n",
    "from dgl.data.utils import save_graphs\n",
    "from tqdm import tqdm\n",
    "\n",
    "import dgl.backend as F\n",
    "from dgl import graph, heterograph, batch\n",
    "from dgllife.utils.mol_to_graph import k_nearest_neighbors, mol_to_bigraph\n",
    "from dgllife.utils.featurizers import BaseAtomFeaturizer, BaseBondFeaturizer, ConcatFeaturizer, atom_type_one_hot, atom_total_degree_one_hot, atom_formal_charge_one_hot, atom_is_aromatic, atom_implicit_valence_one_hot, atom_explicit_valence_one_hot, bond_type_one_hot, bond_is_in_ring\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "ROOT_DIR = os.getcwd()\n",
    "print(f'Current working directory : {ROOT_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1e653c",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97225cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDBBind_v2020(PDBBind):\n",
    "    def __init__(self, \n",
    "                 subset, \n",
    "                 pdb_version='v2015', \n",
    "                 load_binding_pocket=True, \n",
    "                 remove_coreset_from_refinedset=True, \n",
    "                 sanitize=False, \n",
    "                 calc_charges=False, \n",
    "                 remove_hs=False, \n",
    "                 use_conformation=True,\n",
    "                 construct_graph_and_featurize=ACNN_graph_construction_and_featurization,\n",
    "                 zero_padding=True, \n",
    "                 num_processes=None, \n",
    "                 local_path=None,\n",
    "                 distance_bins=[1.5, 2.5, 3.5, 4.5],\n",
    "                 save_bin_files=True):\n",
    "        print(\" \")\n",
    "        print(\"---Using PDBBind v2020 compatible loader---\")\n",
    "        print(\" \")\n",
    "        \n",
    "        self.save_bin_files = save_bin_files\n",
    "        self.pdb_version = pdb_version\n",
    "        self.subset = subset\n",
    "        self.distance_bins = distance_bins\n",
    "        \n",
    "        super().__init__(subset, \n",
    "                         pdb_version, \n",
    "                         load_binding_pocket, \n",
    "                         remove_coreset_from_refinedset, \n",
    "                         sanitize, \n",
    "                         calc_charges, \n",
    "                         remove_hs, \n",
    "                         use_conformation,\n",
    "                         construct_graph_and_featurize,\n",
    "                         zero_padding, \n",
    "                         num_processes, \n",
    "                         local_path)\n",
    "        \n",
    "    def _read_data_files(self, pdb_version, subset, load_binding_pocket, remove_coreset_from_refinedset, local_path):\n",
    "        \"\"\"Download and extract pdbbind data files specified by the version\"\"\"\n",
    "        root_dir_path = get_download_dir()\n",
    "        if local_path:\n",
    "            print(\" \")\n",
    "            print(\"--Using Local Path--\")\n",
    "            print(\" \")\n",
    "            if local_path[-1] != '/':\n",
    "                local_path += '/'\n",
    "            index_label_file = glob.glob(local_path + '*' + subset + '*data*')[0]\n",
    "            \n",
    "        elif pdb_version == 'v2015':\n",
    "            print(\" \")\n",
    "            print(\"--v2015--\")\n",
    "            print(\" \")\n",
    "            \n",
    "            self._url = 'dataset/pdbbind_v2015.tar.gz'\n",
    "            data_path = root_dir_path + '/pdbbind_v2015.tar.gz'\n",
    "            extracted_data_path = root_dir_path + '/pdbbind_v2015'\n",
    "            download(_get_dgl_url(self._url), path=data_path, overwrite=False)\n",
    "            extract_archive(data_path, extracted_data_path)\n",
    "\n",
    "            if subset == 'core':\n",
    "                index_label_file = extracted_data_path + '/v2015/INDEX_core_data.2013'\n",
    "            elif subset == 'refined':\n",
    "                index_label_file = extracted_data_path + '/v2015/INDEX_refined_data.2015'\n",
    "            else:\n",
    "                raise ValueError('Expect the subset_choice to be either core or refined, got {}'.format(subset))\n",
    "                \n",
    "        elif pdb_version == 'v2020':\n",
    "            print(\" \")\n",
    "            print(\"--v2020--\")\n",
    "            print(\" \")\n",
    "            \n",
    "            root_dir_path = ROOT_DIR + f'/{data_dir}'\n",
    "            print(f'root_dir_path : {root_dir_path}')\n",
    "            \n",
    "            # Others\n",
    "            #print(\"--v2020 other PL--\")\n",
    "            #\n",
    "            #data_path = root_dir_path + '/PDBbind_v2020_other_PL.tar.gz'\n",
    "            #print(f'data_path : {data_path}')\n",
    "            #extracted_data_path = root_dir_path + '/pdbbind_v2020'\n",
    "            #extract_archive(data_path, extracted_data_path)\n",
    "            \n",
    "            # Refined\n",
    "            print(\"--v2020 refined--\")\n",
    "            data_path = root_dir_path + '/PDBbind_v2020_refined.tar.gz'\n",
    "            print(f'data_path : {data_path}')\n",
    "            extracted_data_path = root_dir_path + '/pdbbind_v2020'\n",
    "            extract_archive(data_path, extracted_data_path, overwrite=True)\n",
    "\n",
    "            #index_label_file = extracted_data_path + '/v2020-other-PL/index/INDEX_general_PL_data.2020'\n",
    "            if subset == 'core':\n",
    "                print(\"--v2020 core--\")\n",
    "                # Read index file for refined dataset \n",
    "                #index_label_file = extracted_data_path + '/v2020-other-PL/index/INDEX_refined_data.2020'\n",
    "                index_label_file = extracted_data_path + '/refined-set/index/INDEX_refined_data.2020'\n",
    "                # Core\n",
    "                data_path = root_dir_path + '/CASF-2016.tar.gz'\n",
    "                extracted_data_path = root_dir_path + '/pdbbind_v2020'\n",
    "                extract_archive(data_path, extracted_data_path, overwrite=True)\n",
    "                core_dir = extracted_data_path + '/CASF-2016/coreset'\n",
    "                print(f'core_dir : {core_dir}')\n",
    "            elif subset == 'refined':\n",
    "                #index_label_file = extracted_data_path + '/v2020-other-PL/index/INDEX_refined_data.2020'\n",
    "                index_label_file = extracted_data_path + '/refined-set/index/INDEX_refined_data.2020'\n",
    "            elif subset == 'general':\n",
    "                #index_label_file = extracted_data_path + '/v2020-other-PL/index/INDEX_general_PL_data.2020'\n",
    "                index_label_file = extracted_data_path + '/refined-set/index/INDEX_general_PL_data.2020'\n",
    "                \n",
    "        elif pdb_version == 'v2007':\n",
    "            print(\" \")\n",
    "            print(\"--v2007--\")\n",
    "            print(\" \")\n",
    "            \n",
    "            self._url = 'dataset/pdbbind_v2007.tar.gz'\n",
    "            data_path = root_dir_path + '/pdbbind_v2007.tar.gz'\n",
    "            extracted_data_path = root_dir_path + '/pdbbind_v2007'\n",
    "            download(_get_dgl_url(self._url), path=data_path, overwrite=False)\n",
    "            extract_archive(data_path, extracted_data_path, overwrite=False)\n",
    "            extracted_data_path += '/home/ubuntu' # extra layer \n",
    "\n",
    "            # DataFrame containing the pdbbind_2007_agglomerative_split.txt\n",
    "            self.agg_split = pd.read_csv(extracted_data_path + '/v2007/pdbbind_2007_agglomerative_split.txt')\n",
    "            self.agg_split.rename(columns={'PDB ID':'PDB_code', 'Sequence-based assignment':'sequence', 'Structure-based assignment':'structure'}, inplace=True)\n",
    "            self.agg_split.loc[self.agg_split['PDB_code']=='1.00E+66', 'PDB_code'] = '1e66' # fix typo\n",
    "            if subset == 'core':\n",
    "                index_label_file = extracted_data_path + '/v2007/INDEX.2007.core.data'\n",
    "            elif subset == 'refined':\n",
    "                index_label_file = extracted_data_path + '/v2007/INDEX.2007.refined.data'\n",
    "            else:\n",
    "                raise ValueError('Expect the subset_choice to be either core or refined, got {}'.format(subset))\n",
    "                \n",
    "                \n",
    "        print(\"\")         \n",
    "        print(\"index_label_file\")        \n",
    "        print(index_label_file)\n",
    "        print(\"\") \n",
    "        contents = []\n",
    "        with open(index_label_file, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                if line[0] != \"#\":\n",
    "                    splitted_elements = line.split()\n",
    "                    if pdb_version == 'v2015' or pdb_version == 'v2020':\n",
    "                        if len(splitted_elements) == 8:\n",
    "                            # Ignore \"//\"\n",
    "                            contents.append(splitted_elements[:5] + splitted_elements[6:])\n",
    "                        else:\n",
    "                            print('Incorrect data format.')\n",
    "                            print(splitted_elements)\n",
    "\n",
    "                    elif pdb_version == 'v2007':\n",
    "                        if len(splitted_elements) == 6:\n",
    "                            contents.append(splitted_elements)\n",
    "                        else:\n",
    "                            contents.append(splitted_elements[:5] + [' '.join(splitted_elements[5:])])\n",
    "\n",
    "        if pdb_version == 'v2015' or pdb_version == 'v2020':\n",
    "            self.df = pd.DataFrame(contents, columns=(\n",
    "                'PDB_code', 'resolution', 'release_year',\n",
    "                '-logKd/Ki', 'Kd/Ki', 'reference', 'ligand_name'))\n",
    "        elif pdb_version == 'v2007':\n",
    "            self.df = pd.DataFrame(contents, columns=(\n",
    "                'PDB_code', 'resolution', 'release_year',\n",
    "                '-logKd/Ki', 'Kd/Ki', 'cluster_ID'))\n",
    "         \n",
    "        if local_path:\n",
    "            pdb_path = local_path\n",
    "        elif pdb_version == 'v2020' and subset != 'core':\n",
    "            pdb_path = os.path.join(extracted_data_path, 'v2020-other-PL')\n",
    "            print('Loading PDBBind data from', pdb_path)\n",
    "            pdb_dirs = glob.glob(pdb_path + '/*')\n",
    "            \n",
    "            pdb_path = os.path.join(extracted_data_path, 'refined-set')\n",
    "            print('Loading PDBBind data from', pdb_path)\n",
    "            pdb_dirs += glob.glob(pdb_path + '/*')\n",
    "        elif pdb_version == 'v2020' and subset == 'core':\n",
    "            pdb_path = os.path.join(extracted_data_path, 'CASF-2016/coreset')\n",
    "            print('Loading PDBBind data from', pdb_path)\n",
    "            pdb_dirs = glob.glob(pdb_path + '/*')\n",
    "            \n",
    "        else:\n",
    "            pdb_path = os.path.join(extracted_data_path, pdb_version)\n",
    "            print('Loading PDBBind data from', pdb_path)\n",
    "            pdb_dirs = glob.glob(pdb_path + '/*')\n",
    "        \n",
    "        pdb_dirs = [pdb_dir for pdb_dir in pdb_dirs if '.' not in pdb_dir.split('/')[-1]]\n",
    "        \n",
    "        print(\"\")\n",
    "        print('pdb_dirs from dirs')\n",
    "        print(f\"data length : {len(pdb_dirs)}\")\n",
    "        print(\"\")\n",
    "        \n",
    "        ## pdb and pdb_dirs update\n",
    "        dict_pdb_dirs = {pdb_dir.split('/')[-1]: pdb_dir for pdb_dir in pdb_dirs}\n",
    "        self.df['pdb_paths'] = self.df['PDB_code'].map(dict_pdb_dirs)  \n",
    "        self.df = self.df.dropna().drop_duplicates().reset_index().drop(columns=['index'])\n",
    "        \n",
    "        # remove core set from refined set if using refined\n",
    "        if remove_coreset_from_refinedset and subset == 'refined' and pdb_version != 'v2020':\n",
    "            if local_path:\n",
    "                core_path = glob.glob(local_path + '*core*data*')[0]\n",
    "            elif pdb_version == 'v2015':\n",
    "                core_path = extracted_data_path + '/v2015/INDEX_core_data.2013'\n",
    "            elif pdb_version == 'v2007':\n",
    "                core_path = extracted_data_path + '/v2007/INDEX.2007.core.data'\n",
    "\n",
    "            core_pdbs = []\n",
    "            with open(core_path,'r') as f:\n",
    "                for line in f:\n",
    "                    fields = line.strip().split()\n",
    "                    if fields[0] != \"#\":\n",
    "                        core_pdbs.append(fields[0])\n",
    "                        \n",
    "            non_core_ids = []\n",
    "            for i in range(len(self.df)):\n",
    "                if self.df['PDB_code'][i] not in core_pdbs:\n",
    "                    non_core_ids.append(i)\n",
    "            self.df = self.df.iloc[non_core_ids]\n",
    "                        \n",
    "        if remove_coreset_from_refinedset and subset != 'core' and pdb_version == 'v2020':\n",
    "            core_pdb_path = os.path.join(extracted_data_path, 'CASF-2016/coreset')\n",
    "            print('Loading PDBBind data from', core_pdb_path)\n",
    "            core_pdb_dirs = glob.glob(core_pdb_path + '/*')\n",
    "            \n",
    "            core_pdb_dirs = [core_pdb_dir for core_pdb_dir in core_pdb_dirs if '.' not in core_pdb_dir]\n",
    "            \n",
    "            core_pdbs = []\n",
    "            for core_pdb in core_pdb_dirs: \n",
    "\n",
    "                core_pdbs.append(core_pdb.split('/')[-1])\n",
    "\n",
    "            non_core_ids = []\n",
    "            for i in range(len(self.df)):\n",
    "                if self.df['PDB_code'][i] not in core_pdbs:\n",
    "                    non_core_ids.append(i)\n",
    "            self.df = self.df.iloc[non_core_ids]\n",
    "            \n",
    "        \n",
    "        # The final version of self.df\n",
    "        pdbs = self.df['PDB_code'].tolist()\n",
    "        pdb_dirs = self.df['pdb_paths'].tolist()\n",
    "\n",
    "        \n",
    "        print(\"\")\n",
    "        print('pdbs')\n",
    "        #print(pdbs)\n",
    "        print(f\"data length : {len(pdbs)}\")\n",
    "        print(\"\")\n",
    "        \n",
    "        print(\"\")\n",
    "        print('pdb_dirs from dirs')\n",
    "        print(f\"data length : {len(pdb_dirs)}\")\n",
    "        print(\"\")\n",
    "\n",
    "        self.ligand_files = [os.path.join(pdb_dir, '{}_ligand.sdf'.format(pdb_dir.split('/')[-1])) for pdb_dir in pdb_dirs if pdb_dir.split('/')[-1] in pdbs]\n",
    "\n",
    "        if load_binding_pocket:\n",
    "            self.protein_files = [os.path.join(pdb_dir, '{}_pocket.pdb'.format(pdb_dir.split('/')[-1])) for pdb_dir in pdb_dirs if pdb_dir.split('/')[-1] in pdbs]\n",
    "        else:\n",
    "            self.protein_files = [os.path.join(pdb_dir, '{}_protein.pdb'.format(pdb_dir.split('/')[-1])) for pdb_dir in pdb_dirs if pdb_dir.split('/')[-1] in pdbs]\n",
    "        \n",
    "    def _preprocess(self, load_binding_pocket,\n",
    "                    sanitize, calc_charges, remove_hs, use_conformation,\n",
    "                    construct_graph_and_featurize, zero_padding, num_processes):\n",
    "        \"\"\"Preprocess the dataset.\n",
    "\n",
    "        The pre-processing proceeds as follows:\n",
    "\n",
    "        1. Load the dataset\n",
    "        2. Clean the dataset and filter out invalid pairs\n",
    "        3. Construct graphs\n",
    "        4. Prepare node and edge features\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        load_binding_pocket : bool\n",
    "            Whether to load binding pockets or full proteins.\n",
    "        sanitize : bool\n",
    "            Whether sanitization is performed in initializing RDKit molecule instances. See\n",
    "            https://www.rdkit.org/docs/RDKit_Book.html for details of the sanitization.\n",
    "        calc_charges : bool\n",
    "            Whether to add Gasteiger charges via RDKit. Setting this to be True will enforce\n",
    "            ``sanitize`` to be True.\n",
    "        remove_hs : bool\n",
    "            Whether to remove hydrogens via RDKit. Note that removing hydrogens can be quite\n",
    "            slow for large molecules.\n",
    "        use_conformation : bool\n",
    "            Whether we need to extract molecular conformation from proteins and ligands.\n",
    "        construct_graph_and_featurize : callable\n",
    "            Construct a DGLHeteroGraph for the use of GNNs. Mapping self.ligand_mols[i],\n",
    "            self.protein_mols[i], self.ligand_coordinates[i] and self.protein_coordinates[i]\n",
    "            to a DGLHeteroGraph. Default to :func:`ACNN_graph_construction_and_featurization`.\n",
    "        zero_padding : bool\n",
    "            Whether to perform zero padding. While DGL does not necessarily require zero padding,\n",
    "            pooling operations for variable length inputs can introduce stochastic behaviour, which\n",
    "            is not desired for sensitive scenarios.\n",
    "        num_processes : int or None\n",
    "            Number of worker processes to use. If None,\n",
    "            then we will use the number of CPUs in the system.\n",
    "        \"\"\"\n",
    "        if num_processes is None:\n",
    "            num_processes = multiprocessing.cpu_count()\n",
    "        num_processes = min(num_processes, len(self.df))\n",
    "\n",
    "        print('Loading ligands...')\n",
    "        ligands_loaded = multiprocess_load_molecules(self.ligand_files,\n",
    "                                                     sanitize=sanitize,\n",
    "                                                     calc_charges=calc_charges,\n",
    "                                                     remove_hs=remove_hs,\n",
    "                                                     use_conformation=use_conformation,\n",
    "                                                     num_processes=num_processes)\n",
    "\n",
    "        print('Loading proteins...')\n",
    "        proteins_loaded = multiprocess_load_molecules(self.protein_files,\n",
    "                                                      sanitize=sanitize,\n",
    "                                                      calc_charges=calc_charges,\n",
    "                                                      remove_hs=remove_hs,\n",
    "                                                      use_conformation=use_conformation,\n",
    "                                                      num_processes=num_processes)\n",
    "\n",
    "        self._filter_out_invalid(ligands_loaded, proteins_loaded, use_conformation)\n",
    "        self.df = self.df.iloc[self.indices]\n",
    "                \n",
    "        self.labels = F.zerocopy_from_numpy(self.df[self.task_names].values.astype(np.float32))\n",
    "        print('Finished cleaning the dataset, '\n",
    "              'got {:d}/{:d} valid pairs'.format(len(self), len(self.ligand_files))) # account for the ones use_conformation failed\n",
    "\n",
    "        # Prepare zero padding\n",
    "        if zero_padding:\n",
    "            max_num_ligand_atoms = 0\n",
    "            max_num_protein_atoms = 0\n",
    "            for i in range(len(self)):\n",
    "                max_num_ligand_atoms = max(\n",
    "                    max_num_ligand_atoms, self.ligand_mols[i].GetNumAtoms())\n",
    "                max_num_protein_atoms = max(\n",
    "                    max_num_protein_atoms, self.protein_mols[i].GetNumAtoms())\n",
    "        else:\n",
    "            max_num_ligand_atoms = None\n",
    "            max_num_protein_atoms = None\n",
    "\n",
    "        construct_graph_and_featurize = partial(construct_graph_and_featurize, \n",
    "                            max_num_ligand_atoms=max_num_ligand_atoms,\n",
    "                            max_num_protein_atoms=max_num_protein_atoms)\n",
    "\n",
    "        print('Start constructing graphs and featurizing them.')\n",
    "        num_mols = len(self)\n",
    "        \n",
    "        # Run this after the filter\n",
    "        pdbs = self.df['PDB_code'].tolist()\n",
    "        pdb_dirs = self.df['pdb_paths'].tolist()\n",
    "        pdb_version = self.pdb_version\n",
    "        subset = self.subset\n",
    "        self.df.to_csv(ROOT_DIR + f'/{pdb_version}-{subset}-{pdbs[0]}-{pdbs[-1]}-{len(pdbs)}.csv')\n",
    "        \n",
    "        \n",
    "        if self.save_bin_files: \n",
    "            for i1 in tqdm(range(len(self.labels)), desc=\"Loading...\"):\n",
    "                PN_graph_construction_and_featurization_and_save(self.ligand_mols[i1],\n",
    "                                                                 self.protein_mols[i1],\n",
    "                                                                 self.ligand_coordinates[i1],\n",
    "                                                                 self.protein_coordinates[i1],\n",
    "                                                                 self.df['PDB_code'].iloc[i1],\n",
    "                                                                 self.labels[i1],\n",
    "                                                                 self.pdb_version,\n",
    "                                                                 self.subset,\n",
    "                                                                 max_num_ligand_atoms=max_num_ligand_atoms,\n",
    "                                                                 max_num_protein_atoms=max_num_protein_atoms,\n",
    "                                                                 max_num_neighbors=4,\n",
    "                                                                 distance_bins=self.distance_bins,\n",
    "                                                                 strip_hydrogens=False)\n",
    "\n",
    "        else:\n",
    "            pool = multiprocessing.Pool(processes=num_processes)\n",
    "            self.graphs = pool.starmap(construct_graph_and_featurize, \n",
    "                                       zip(self.ligand_mols, self.protein_mols,\n",
    "                                           self.ligand_coordinates, self.protein_coordinates))\n",
    "        \n",
    "        print(f'Done constructing {len(self.labels)} graphs.')\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"Get the datapoint associated with the index.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        item : int\n",
    "            Index for the datapoint.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Index for the datapoint.\n",
    "        rdkit.Chem.rdchem.Mol\n",
    "            RDKit molecule instance for the ligand molecule.\n",
    "        rdkit.Chem.rdchem.Mol\n",
    "            RDKit molecule instance for the protein molecule.\n",
    "        DGLGraph or tuple of DGLGraphs\n",
    "            Pre-processed DGLGraph with features extracted.\n",
    "            For ACNN, a single DGLGraph;\n",
    "            For PotentialNet, a tuple of DGLGraphs that consists of a molecular graph and a KNN graph of the complex.\n",
    "        Float32 tensor\n",
    "            Label for the datapoint.\n",
    "        \"\"\"\n",
    "        if self.save_bin_files:\n",
    "            return item, self.ligand_mols[item], self.protein_mols[item], self.labels[item]\n",
    "\n",
    "        else:\n",
    "            return item, self.ligand_mols[item], self.protein_mols[item], \\\n",
    "               self.graphs[item], self.labels[item]\n",
    "    \n",
    "\n",
    "    \n",
    "def PN_graph_construction_and_featurization_and_save(ligand_mol,\n",
    "                                                     protein_mol,\n",
    "                                                     ligand_coordinates,\n",
    "                                                     protein_coordinates,\n",
    "                                                     pdb_id,\n",
    "                                                     label,\n",
    "                                                     pdb_version,\n",
    "                                                     subset,\n",
    "                                                     max_num_ligand_atoms=None,\n",
    "                                                     max_num_protein_atoms=None,\n",
    "                                                     max_num_neighbors=4,\n",
    "                                                     distance_bins=[1.5, 2.5, 3.5, 4.5],\n",
    "                                                     strip_hydrogens=False):\n",
    "    \"\"\"Graph construction and featurization for `PotentialNet for Molecular Property Prediction\n",
    "     <https://pubs.acs.org/doi/10.1021/acscentsci.8b00507>`__.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ligand_mol : rdkit.Chem.rdchem.Mol\n",
    "        RDKit molecule instance.\n",
    "    protein_mol : rdkit.Chem.rdchem.Mol\n",
    "        RDKit molecule instance.\n",
    "    ligand_coordinates : Float Tensor of shape (V1, 3)\n",
    "        Atom coordinates in a ligand.\n",
    "    protein_coordinates : Float Tensor of shape (V2, 3)\n",
    "        Atom coordinates in a protein.\n",
    "    max_num_ligand_atoms : int or None\n",
    "        Maximum number of atoms in ligands for zero padding, which should be no smaller than\n",
    "        ligand_mol.GetNumAtoms() if not None. If None, no zero padding will be performed.\n",
    "        Default to None.\n",
    "    max_num_protein_atoms : int or None\n",
    "        Maximum number of atoms in proteins for zero padding, which should be no smaller than\n",
    "        protein_mol.GetNumAtoms() if not None. If None, no zero padding will be performed.\n",
    "        Default to None.\n",
    "    max_num_neighbors : int\n",
    "        Maximum number of neighbors allowed for each atom when constructing KNN graph. Default to 4.\n",
    "    distance_bins : list of float\n",
    "        Distance bins to determine the edge types.\n",
    "        Edges of the first edge type are added between pairs of atoms whose distances are less than `distance_bins[0]`.\n",
    "        The length matches the number of edge types to be constructed.\n",
    "        Default `[1.5, 2.5, 3.5, 4.5]`.\n",
    "    strip_hydrogens : bool\n",
    "        Whether to exclude hydrogen atoms. Default to False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    complex_bigraph : DGLGraph\n",
    "        Bigraph with the ligand and the protein (pocket) combined and canonical features extracted.\n",
    "        The atom features are stored as DGLGraph.ndata['h'].\n",
    "        The edge types are stored as DGLGraph.edata['e'].\n",
    "        The bigraphs of the ligand and the protein are batched together as one complex graph.\n",
    "    complex_knn_graph : DGLGraph\n",
    "        K-nearest-neighbor graph with the ligand and the protein (pocket) combined and edge features extracted based on distances.\n",
    "        The edge types are stored as DGLGraph.edata['e'].\n",
    "        The knn graphs of the ligand and the protein are batched together as one complex graph.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    assert ligand_coordinates is not None, 'Expect ligand_coordinates to be provided.'\n",
    "    assert protein_coordinates is not None, 'Expect protein_coordinates to be provided.'\n",
    "    if max_num_ligand_atoms is not None:\n",
    "        assert max_num_ligand_atoms >= ligand_mol.GetNumAtoms(), \\\n",
    "            'Expect max_num_ligand_atoms to be no smaller than ligand_mol.GetNumAtoms(), ' \\\n",
    "            'got {:d} and {:d}'.format(max_num_ligand_atoms, ligand_mol.GetNumAtoms())\n",
    "    if max_num_protein_atoms is not None:\n",
    "        assert max_num_protein_atoms >= protein_mol.GetNumAtoms(), \\\n",
    "            'Expect max_num_protein_atoms to be no smaller than protein_mol.GetNumAtoms(), ' \\\n",
    "            'got {:d} and {:d}'.format(max_num_protein_atoms, protein_mol.GetNumAtoms())\n",
    "\n",
    "    if strip_hydrogens:\n",
    "        # Remove hydrogen atoms and their corresponding coordinates\n",
    "        ligand_atom_indices_left = filter_out_hydrogens(ligand_mol)\n",
    "        protein_atom_indices_left = filter_out_hydrogens(protein_mol)\n",
    "        ligand_coordinates = ligand_coordinates.take(ligand_atom_indices_left, axis=0)\n",
    "        protein_coordinates = protein_coordinates.take(protein_atom_indices_left, axis=0)\n",
    "    else:\n",
    "        ligand_atom_indices_left = list(range(ligand_mol.GetNumAtoms()))\n",
    "        protein_atom_indices_left = list(range(protein_mol.GetNumAtoms()))\n",
    "\n",
    "    # Node featurizer for stage 1\n",
    "    atoms = ['H','N','O','C','P','S','F','Br','Cl','I','Fe','Zn','Mg','Na','Mn','Ca','Co','Ni','Se','Cu','Cd','Hg','K']\n",
    "    atom_total_degrees = list(range(5))\n",
    "    atom_formal_charges = [-1, 0, 1]\n",
    "    atom_implicit_valence = list(range(4))\n",
    "    atom_explicit_valence = list(range(8))\n",
    "    atom_concat_featurizer = ConcatFeaturizer([partial(atom_type_one_hot, allowable_set=atoms), \n",
    "                                               partial(atom_total_degree_one_hot, allowable_set=atom_total_degrees),\n",
    "                                               partial(atom_formal_charge_one_hot, allowable_set=atom_formal_charges),\n",
    "                                               atom_is_aromatic,\n",
    "                                               partial(atom_implicit_valence_one_hot, allowable_set=atom_implicit_valence),\n",
    "                                               partial(atom_explicit_valence_one_hot, allowable_set=atom_explicit_valence)])\n",
    "    PN_atom_featurizer = BaseAtomFeaturizer({'h': atom_concat_featurizer})\n",
    "\n",
    "    # Bond featurizer for stage 1\n",
    "    bond_concat_featurizer = ConcatFeaturizer([bond_type_one_hot, bond_is_in_ring])\n",
    "    PN_bond_featurizer = BaseBondFeaturizer({'e': bond_concat_featurizer})\n",
    "\n",
    "    # construct graphs for stage 1\n",
    "    ligand_bigraph = mol_to_bigraph(ligand_mol, add_self_loop=False,\n",
    "                                    node_featurizer=PN_atom_featurizer,\n",
    "                                    edge_featurizer=PN_bond_featurizer,\n",
    "                                    canonical_atom_order=False) # Keep the original atomic order)\n",
    "    protein_bigraph = mol_to_bigraph(protein_mol, add_self_loop=False,\n",
    "                                     node_featurizer=PN_atom_featurizer,\n",
    "                                     edge_featurizer=PN_bond_featurizer,\n",
    "                                     canonical_atom_order=False)\n",
    "    complex_bigraph = batch([ligand_bigraph, protein_bigraph])\n",
    "\n",
    "    # Construct knn graphs for stage 2\n",
    "    complex_coordinates = np.concatenate([ligand_coordinates, protein_coordinates])\n",
    "    complex_srcs, complex_dsts, complex_dists = k_nearest_neighbors(\n",
    "            complex_coordinates, distance_bins[-1], max_num_neighbors)\n",
    "    complex_srcs = np.array(complex_srcs)\n",
    "    complex_dsts = np.array(complex_dsts)\n",
    "    complex_dists = np.array(complex_dists)\n",
    "\n",
    "    complex_knn_graph = graph((complex_srcs, complex_dsts), num_nodes=len(complex_coordinates))\n",
    "    d_features = np.digitize(complex_dists, bins=distance_bins, right=True)\n",
    "    d_one_hot = int_2_one_hot(d_features)\n",
    "    \n",
    "    # add bond types and bonds (from bigraph) to stage 2\n",
    "    u, v = complex_bigraph.edges()    \n",
    "    complex_knn_graph.add_edges(u.to(F.int64), v.to(F.int64))\n",
    "    n_d, f_d = d_one_hot.shape\n",
    "    n_e, f_e = complex_bigraph.edata['e'].shape\n",
    "    complex_knn_graph.edata['e'] = F.zerocopy_from_numpy(\n",
    "        np.block([\n",
    "            [d_one_hot, np.zeros((n_d, f_e))],\n",
    "            [np.zeros((n_e, f_d)), np.array(complex_bigraph.edata['e'])]\n",
    "        ]).astype(np.long)\n",
    "    )\n",
    "    #return complex_bigraph, complex_knn_graph\n",
    "    \n",
    "    e_dims = len(distance_bins) + 5\n",
    "    \n",
    "    graph_dir = ROOT_DIR + f'/graph_files_{pdb_version}_{subset}_{e_dims}_withPDBID/'\n",
    "    isExist = os.path.exists(graph_dir)\n",
    "    \n",
    "    if not isExist: \n",
    "        os.makedirs(graph_dir)\n",
    "        print(f\"The new directory {graph_dir} is created!\") \n",
    "    \n",
    "    #shuffling\n",
    "    seed = 0\n",
    "    shuffle_index = list(range(44))\n",
    "    random.seed(seed)  \n",
    "    random.shuffle(shuffle_index)\n",
    "    for bg in dgl.unbatch(complex_bigraph):\n",
    "        for ndata in bg.ndata['h']:\n",
    "            ndata = ndata[shuffle_index]\n",
    "            \n",
    "    graph_labels = {\"glabel\": torch.FloatTensor(label)}\n",
    "    save_graphs(graph_dir + f\"{pdb_id}_g1.bin\", [bg for bg in dgl.unbatch(complex_bigraph)], graph_labels)\n",
    "    save_graphs(graph_dir + f\"{pdb_id}_g2.bin\", complex_knn_graph, graph_labels)\n",
    "    \n",
    "\n",
    "def int_2_one_hot(a):\n",
    "    \"\"\"Convert integer encodings on a vector to a matrix of one-hot encoding\"\"\"\n",
    "    n = len(a)\n",
    "    b = np.zeros((n, a.max()+1))\n",
    "    b[np.arange(n), a] = 1\n",
    "    return b   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e48635c",
   "metadata": {},
   "source": [
    "## Making Binary File Pairs \n",
    "\n",
    "### Core Dataset\n",
    "\n",
    "We are going to make pairs of graph binary files (e.g. PDB-ID_g1.bin and PDB-ID_g2.bin) and data sheet that describes PDB ID, binding affinity, and local path to PDB files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bcbe3c",
   "metadata": {},
   "source": [
    "The variable, **distance_bins**, needs to be set here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fb1388",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "\n",
    "args['subset'] = 'core'\n",
    "args['version'] = 'v2020' \n",
    "args['remove_coreset_from_refinedset'] = True\n",
    "args['load_binding_pocket'] = True\n",
    "args['num_workers'] = 0\n",
    "args['save_bin_files'] = True   \n",
    "args['distance_bins'] =  [1.5, 2.5, 2.7, 2.9, 3.1, 3.3, 3.5, 4.5]\n",
    "\n",
    "if args['num_workers'] == 0:\n",
    "    args['num_workers'] = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69e8343",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "PDBBind_v2020(subset=args['subset'], \n",
    "              pdb_version=args['version'], \n",
    "              remove_coreset_from_refinedset=args['remove_coreset_from_refinedset'],\n",
    "              load_binding_pocket=args['load_binding_pocket'],\n",
    "              num_processes=args['num_workers'],\n",
    "              construct_graph_and_featurize=partial(PN_graph_construction_and_featurization, \n",
    "              distance_bins=args['distance_bins']),\n",
    "              save_bin_files=args['save_bin_files'],\n",
    "              distance_bins=args['distance_bins'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65c0c99",
   "metadata": {},
   "source": [
    "### Refined Dataset\n",
    "\n",
    "Likewise, this is for refined dataset. By specifying **args['remove_coreset_from_refinedset'] = True**, PDB IDs that appear in the core dataset are excluded from the data processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4766760",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "args['subset'] = 'refined'\n",
    "\n",
    "PDBBind_v2020(subset=args['subset'], \n",
    "              pdb_version=args['version'], \n",
    "              remove_coreset_from_refinedset=args['remove_coreset_from_refinedset'],\n",
    "              load_binding_pocket=args['load_binding_pocket'],\n",
    "              num_processes=args['num_workers'],\n",
    "              construct_graph_and_featurize=partial(PN_graph_construction_and_featurization, \n",
    "              distance_bins=args['distance_bins']),\n",
    "              save_bin_files=args['save_bin_files'],\n",
    "              distance_bins=args['distance_bins'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c182d528",
   "metadata": {},
   "source": [
    "### General Dataset\n",
    "\n",
    "It should be noted that PDB ids in refined dataset is included in the general dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a107a069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#args['subset'] = 'general'\n",
    "#\n",
    "#PDBBind_v2020(subset=args['subset'], \n",
    "#              pdb_version=args['version'], \n",
    "#              remove_coreset_from_refinedset=args['remove_coreset_from_refinedset'],\n",
    "#              load_binding_pocket=args['load_binding_pocket'],\n",
    "#              num_processes=args['num_workers'],\n",
    "#              construct_graph_and_featurize=partial(PN_graph_construction_and_featurization, \n",
    "#              distance_bins=args['distance_bins']),\n",
    "#              save_bin_files=args['save_bin_files'],\n",
    "#              distance_bins=args['distance_bins'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe841a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "312eebbb",
   "metadata": {},
   "source": [
    "## Split dataset\n",
    "\n",
    "In this example, we are going to combine **core** and **general** datasets, and splits the combined dataset into train, validation, test and holdout datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d1e7c8",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651fd6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from dgl.data.utils import load_graphs\n",
    "import glob\n",
    "\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from itertools import accumulate, chain\n",
    "\n",
    "import numpy as np\n",
    "import dgl.backend as F\n",
    "from dgl.data.utils import split_dataset, Subset\n",
    "\n",
    "try:\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import rdMolDescriptors\n",
    "    from rdkit.Chem.rdmolops import FastFindRings\n",
    "    from rdkit.Chem import AllChem\n",
    "    from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "from dgllife.utils import (RandomSplitter, ScaffoldSplitter,\n",
    "                           SingleTaskStratifiedSplitter)#\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, lst_graph1_paths):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.len = len(lst_graph1_paths)\n",
    "        self.lst = lst_graph1_paths\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        graphs1, label_dict = load_graphs(self.lst[index])\n",
    "        graphs2, label_dict = load_graphs(self.lst[index].replace('_g1.bin', '_g2.bin'))\n",
    "        label = label_dict['glabel']\n",
    "        \n",
    "        graphs1_batch = [dgl.batch([graphs1[i],graphs1[i+1]]) for i in range(0, int(len(graphs1)), 2)]\n",
    "        bg = [tuple([graphs1_batch[i],graphs2[i]]) for i in range(0, int(len(graphs2)), 1)]\n",
    "        \n",
    "        return bg[0], label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d127a4",
   "metadata": {},
   "source": [
    "We are going to make a list of g1.bin file names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1a9f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2cf584",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dir_core_13 = f'{ROOT_DIR}/graph_files_v2020_core_13_withPDBID/'\n",
    "graph_dir_refined_13 = f'{ROOT_DIR}/graph_files_v2020_refined_13_withPDBID/'\n",
    "#graph_dir_general_13 = f'{ROOT_DIR}/graph_files_v2020_general_13_withPDBID/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472ac970",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_g1_core_13 = glob.glob(graph_dir_core_13 + '**_g1.bin')\n",
    "lst_g1_core_13.sort()\n",
    "#lst_g1_core_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27809d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_g1_refined_13 = glob.glob(graph_dir_refined_13 + '**_g1.bin')\n",
    "lst_g1_refined_13.sort()\n",
    "#lst_g1_refined_13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdae96c7",
   "metadata": {},
   "source": [
    "Refined data are used for training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4bf5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_g1_raw = lst_g1_refined_13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a064bd7",
   "metadata": {},
   "source": [
    "## PDB file paths (Scaffold Split)\n",
    "\n",
    "ligand files (.sdf) are used for Scaffold split. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99616a16",
   "metadata": {},
   "source": [
    "We are going to read a data sheet for each dataset. Scaffold split requires pdb files (.sdf). Their paths are described in the data sheet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e1c1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefe953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_266_files = pd.read_csv(f'{ROOT_DIR}/v2020-core-3ao4-4f3c-266.csv')\n",
    "refined_5050_files = pd.read_csv(f'{ROOT_DIR}/v2020-refined-2r58-6e9a-5050.csv')\n",
    "#general_19138_files = pd.read_csv(f'{ROOT_DIR}/v2020-general-3zzf-2avi-19138.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8285d194",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_core_266_files = core_266_files.sort_values('PDB_code')['pdb_paths'].tolist()\n",
    "#lst_core_266_files.sort()\n",
    "#lst_core_266_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd6c4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_refined_5050_files = refined_5050_files.sort_values('PDB_code')['pdb_paths'].tolist()\n",
    "#lst_refined_19138_files.sort()\n",
    "#lst_refined_19138_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad03114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure there are no duplicates\n",
    "\n",
    "assert len(core_266_files['PDB_code']) == len(core_266_files['PDB_code'].drop_duplicates())\n",
    "assert len(refined_5050_files['PDB_code']) == len(refined_5050_files['PDB_code'].drop_duplicates())\n",
    "#assert len(general_19138_files['PDB_code']) == len(general_19138_files['PDB_code'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8c566c",
   "metadata": {},
   "source": [
    "Mol data are loaded using _ligand.sdf files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d598b799",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_mols_raw = []\n",
    "lst_mol_names_raw =lst_refined_5050_files\n",
    "for i0 in range(len(lst_mol_names_raw)):\n",
    "    supplier = Chem.SDMolSupplier(lst_mol_names_raw[i0] + '/' + lst_mol_names_raw[i0].split('/')[-1] + '_ligand.sdf',\n",
    "                                  sanitize=False, removeHs=False)\n",
    "    lst_mols_raw.append(supplier[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be287a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "supplier[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f4b5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(lst_mol_names_raw) == len(lst_g1_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d8e0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make sure the order of PDB ids in two lists matches\n",
    "\n",
    "for i1 in range(len(lst_mol_names_raw)):\n",
    "    assert lst_mol_names_raw[i1].split('/')[-1] == lst_g1_raw[i1].split('/')[-1].split('_')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9215eb",
   "metadata": {},
   "source": [
    "## Data Sanity Chesk\n",
    "\n",
    "Some graphs do not have the proper node or edge dimensions. These graphs are removed from the dataset. \n",
    "\n",
    "This error must be fixed in the latest version of dgl-lifesci. \n",
    "https://github.com/awslabs/dgl-lifesci/pull/170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e2b1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw = MyDataset(lst_g1_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92ce500",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_g1_updated = []\n",
    "lst_mols_updated = []\n",
    "lst_mol_names_updated = []\n",
    "i_count = 0\n",
    "for i0, g in enumerate(dataset_raw):\n",
    "    if g[0][0].ndata['h'].shape[1] != 44 or \\\n",
    "        g[0][0].edata['e'].shape[1] != 5 or \\\n",
    "        g[0][1].edata['e'].shape[1] != 5 + len(args['distance_bins']): # This was fixed with PR 170\n",
    "        i_count += 1\n",
    "        print(i0, i_count, lst_g1_raw[i0])\n",
    "    else:\n",
    "        lst_g1_updated.append(lst_g1_raw[i0])\n",
    "        lst_mols_updated.append(lst_mols_raw[i0])\n",
    "        lst_mol_names_updated.append(lst_mol_names_raw[i0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11e67f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f9c1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450a148f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74f5eee8",
   "metadata": {},
   "source": [
    "## Scaffold Split\n",
    "\n",
    "We are going to make csv files that map each pdb id to different splits (train, val, test, or additional holdout). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a5fbe8",
   "metadata": {},
   "source": [
    "### Perform Bemis-Murcko Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7e7d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(lst_g1_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2417de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaffold_sets = ScaffoldSplitter.get_ordered_scaffold_sets(lst_mols_updated, 1000, 'decompose')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2196240a",
   "metadata": {},
   "source": [
    "### Train - Val - Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb9d3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_train=0.60\n",
    "frac_val=0.20\n",
    "frac_test=0.20\n",
    "\n",
    "train_indices, val_indices, test_indices = [], [], []\n",
    "train_cutoff = int(frac_train * len(lst_mols_updated))\n",
    "val_cutoff = int((frac_train + frac_val) * len(lst_mols_updated))\n",
    "for group_indices in scaffold_sets:\n",
    "    if len(train_indices) + len(group_indices) > train_cutoff:\n",
    "        if len(train_indices) + len(val_indices) + len(group_indices) > val_cutoff:\n",
    "            test_indices.extend(group_indices)\n",
    "        else:\n",
    "            val_indices.extend(group_indices)\n",
    "    else:\n",
    "        train_indices.extend(group_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d6652d",
   "metadata": {},
   "source": [
    "### Train - Val - Test - Holdout Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10013321",
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_train=0.60\n",
    "frac_val=0.10\n",
    "frac_test=0.10\n",
    "frac_finetune=0.20\n",
    "\n",
    "train_indices, val_indices, test_indices, finetune_indices = [], [], [], []\n",
    "train_cutoff = int(frac_train * len(lst_mols_updated))\n",
    "val_cutoff = int((frac_train + frac_val) * len(lst_mols_updated))\n",
    "test_cutoff = int((frac_train + frac_val + frac_test) * len(lst_mols_updated))\n",
    "\n",
    "for group_indices in scaffold_sets:\n",
    "    if len(train_indices) + len(group_indices) > train_cutoff:\n",
    "        if len(train_indices) + len(val_indices) + len(group_indices) > val_cutoff:\n",
    "            if len(train_indices) + len(val_indices) + len(test_indices) + len(group_indices) > test_cutoff:\n",
    "                finetune_indices.extend(group_indices)\n",
    "            else:\n",
    "                test_indices.extend(group_indices)\n",
    "        else:\n",
    "            val_indices.extend(group_indices)\n",
    "    else:\n",
    "        train_indices.extend(group_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e7ce6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_pdb_g1_bin_train = [lst_g1_updated[i].split('/')[-1] for i in train_indices]\n",
    "lst_pdb_g1_bin_val = [lst_g1_updated[i].split('/')[-1] for i in val_indices]\n",
    "lst_pdb_g1_bin_test = [lst_g1_updated[i].split('/')[-1] for i in test_indices]\n",
    "lst_pdb_g1_bin_fine_tune = [lst_g1_updated[i].split('/')[-1] for i in finetune_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a011e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_pdb_g1_bin_train2 = [{'g1_bin': pdb, 'split':'train'} for pdb in lst_pdb_g1_bin_train]\n",
    "lst_pdb_g1_bin_val2 = [{'g1_bin': pdb, 'split':'val'} for pdb in lst_pdb_g1_bin_val]\n",
    "lst_pdb_g1_bin_test2 = [{'g1_bin': pdb, 'split':'test'} for pdb in lst_pdb_g1_bin_test]\n",
    "lst_pdb_g1_bin_fine_tune2 = [{'g1_bin': pdb, 'split':'fine_tune'} for pdb in lst_pdb_g1_bin_fine_tune]\n",
    "\n",
    "lst_pdb_g1_bin2 = lst_pdb_g1_bin_train2 + lst_pdb_g1_bin_val2 + lst_pdb_g1_bin_test2 + lst_pdb_g1_bin_fine_tune2\n",
    "\n",
    "pd.DataFrame(lst_pdb_g1_bin2).to_csv(f'Scaffold-Split-v2020-{len(lst_pdb_g1_bin2)}-files-tvtf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa91088",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lst_pdb_g1_bin2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f05d38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(f'Scaffold-Split-v2020-{len(lst_pdb_g1_bin2)}-files-tvtf.csv')[pd.read_csv(f'Scaffold-Split-v2020-{len(lst_pdb_g1_bin2)}-files-tvtf.csv')['split'] == 'train']['g1_bin'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6ce39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "# Setup session\n",
    "sess = sagemaker.Session()\n",
    "your_bucket = sess.default_bucket()\n",
    "prefix_ = 'kdd-graph-med' #e.g. prefix_ = 'tarai'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe182921",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_bucket_and_prefix = your_bucket + '/' + prefix_\n",
    "your_bucket_and_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cafdb07",
   "metadata": {},
   "source": [
    "## Upload to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acda2427",
   "metadata": {},
   "source": [
    "### Upload core data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3fadec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp graph_files_v2020_core_13_withPDBID s3://{your_bucket}/preprocessed/graph_files_v2020_refined_core/ --recursive --exclude \"*\" --include \"*.bin\" --acl bucket-owner-full-control\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae6d0e1",
   "metadata": {},
   "source": [
    "### Upload general data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b42a125",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp graph_files_v2020_refined_13_withPDBID s3://{your_bucket}/dgl-pdbbind/preprocessed/graph_files_v2020_refined_core/ --recursive --exclude \"*\" --include \"*.bin\" --acl bucket-owner-full-control\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e05fba2",
   "metadata": {},
   "source": [
    "### Upload the split instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad50bb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!aws s3 cp Scaffold-Split-v2020-{len(lst_pdb_g1_bin2)}-files-tvtf.csv s3://{your_bucket}/dgl-pdbbind/preprocessed/graph_files_v2020_refined_core/Scaffold-Split-v2020-{len(lst_pdb_g1_bin2)}-files-tvtf.csv --acl bucket-owner-full-control\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c404acb",
   "metadata": {},
   "source": [
    "## Go to the second notebook : 2_train_potentialnet.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d166f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c779b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd375a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9057f9a6",
   "metadata": {},
   "source": [
    "## (Optional) Pseudo fine tune dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d608c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fineTune = pd.DataFrame(lst_pdb_g1_bin2)[pd.DataFrame(lst_pdb_g1_bin2)['split'] != 'train']\n",
    "df_fineTune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87d72cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fineTune[df_fineTune['split'] == 'fine_tune']['split'] = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a246553",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fineTune['split'] = df_fineTune['split'].replace('fine_tune', 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cd2ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fineTune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8d979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fineTune.to_csv(f'v2020-pseudo-fine-tune.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328e92f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp graph_files_v2020_core_13_withPDBID s3://'<your S3 prefix you want to upload the preprocessed PDBbind dataset>'/dgl-pdbbind/preprocessed/graph_files_v2020_fine_tune/ --recursive --exclude \"*\" --include \"*.bin\" --acl bucket-owner-full-control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2322ebbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp graph_files_v2020_general_13_withPDBID s3://'<your S3 prefix you want to upload the preprocessed PDBbind dataset>'/dgl-pdbbind/preprocessed/graph_files_v2020_fine_tune/ --recursive --exclude \"*\" --include \"*.bin\" --acl bucket-owner-full-control\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a024e09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp v2020-pseudo-fine-tune.csv s3://'<your S3 prefix you want to upload the preprocessed PDBbind dataset>'/dgl-pdbbind/preprocessed/graph_files_v2020_fine_tune/v2020-pseudo-fine-tune.csv --acl bucket-owner-full-control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f1f603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
